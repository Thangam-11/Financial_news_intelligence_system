{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8414f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51be2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f8fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSIngestor:\n",
    "\n",
    "    def __init__(self, feeds: list):\n",
    "        self.feeds = feeds\n",
    "\n",
    "    def fetch(self):\n",
    "        articles = []\n",
    "\n",
    "        for url in self.feeds:\n",
    "            feed = feedparser.parse(url)\n",
    "            \n",
    "\n",
    "            for entry in feed.entries:\n",
    "                articles.append({\n",
    "                    \"title\": entry.title,\n",
    "                    \"content\": entry.summary if hasattr(entry, \"summary\") else \"\",\n",
    "                    \"link\": entry.link,\n",
    "                    \"source\": \"RSS\",\n",
    "                    \"published_at\": datetime(*entry.published_parsed[:6])\n",
    "                        if hasattr(entry, \"published_parsed\")\n",
    "                        else datetime.utcnow()\n",
    "                })\n",
    "\n",
    "        return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c1958e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIIngestor:\n",
    "\n",
    "    def __init__(self, endpoints: list):\n",
    "        self.endpoints = endpoints\n",
    "\n",
    "    def fetch(self):\n",
    "        articles = []\n",
    "\n",
    "        for url in self.endpoints:\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=10)\n",
    "                data = resp.json()\n",
    "\n",
    "                for item in data.get(\"articles\", []):\n",
    "                    articles.append({\n",
    "                        \"title\": item.get(\"title\"),\n",
    "                        \"content\": item.get(\"content\", \"\"),\n",
    "                        \"link\": item.get(\"url\"),\n",
    "                        \"source\": \"API\",\n",
    "                        \"published_at\": datetime.utcnow()\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"API Error:\", e)\n",
    "\n",
    "        return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb4b6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperIngestor:\n",
    "\n",
    "    def __init__(self, pages: list):\n",
    "        self.pages = pages\n",
    "\n",
    "    def fetch(self):\n",
    "        articles = []\n",
    "\n",
    "        for url in self.pages:\n",
    "            try:\n",
    "                resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "                title = soup.find(\"h1\")\n",
    "                content = soup.find(\"article\") or soup.find(\"p\")\n",
    "\n",
    "                if title and content:\n",
    "                    articles.append({\n",
    "                        \"title\": title.get_text(strip=True),\n",
    "                        \"content\": content.get_text(strip=True),\n",
    "                        \"link\": url,\n",
    "                        \"source\": \"SCRAPER\",\n",
    "                        \"published_at\": datetime.utcnow()\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Scraper Error:\", e)\n",
    "\n",
    "        return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b732b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IngestionAgent:\n",
    "\n",
    "    def __init__(self, rss_feeds=None, api_endpoints=None, scraper_pages=None):\n",
    "\n",
    "        self.rss = RSSIngestor(rss_feeds or [])\n",
    "        self.api = APIIngestor(api_endpoints or [])\n",
    "        self.scraper = ScraperIngestor(scraper_pages or [])\n",
    "\n",
    "    def run(self):\n",
    "        all_articles = []\n",
    "\n",
    "        all_articles.extend(self.rss.fetch())\n",
    "        all_articles.extend(self.api.fetch())\n",
    "        all_articles.extend(self.scraper.fetch())\n",
    "\n",
    "        print(f\"Total articles fetched: {len(all_articles)}\")\n",
    "        return all_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f047d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_feeds = [\n",
    "    \"https://economictimes.indiatimes.com/markets/rssfeeds/1977021501.cms\",\n",
    "    \"https://www.moneycontrol.com/rss/latestnews.xml\",\n",
    "    \"https://feeds.finance.yahoo.com/rss/2.0/headline?s=AAPL&region=US&lang=en-US\",\n",
    "    \"https://www.business-standard.com/rss/latest.rss\"\n",
    "]\n",
    "\n",
    "api_key = \"36b5d90eea8cd28253581f3f536871f5\"   # your actual API key\n",
    "\n",
    "api_endpoints = [\n",
    "    f\"https://gnews.io/api/v4/top-headlines?apikey={api_key}&topic=business&lang=en&max=10\",\n",
    "    f\"https://gnews.io/api/v4/top-headlines?apikey={api_key}&topic=world&lang=en&max=10\",\n",
    "    f\"https://gnews.io/api/v4/top-headlines?apikey={api_key}&topic=breaking-news&lang=en&max=10\",\n",
    "    f\"https://gnews.io/api/v4/top-headlines?apikey={api_key}&topic=finance&lang=en&max=10\"\n",
    "]\n",
    "\n",
    "scraper_pages = [\n",
    "    \"https://www.reuters.com/markets/\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a39d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles fetched: 125\n"
     ]
    }
   ],
   "source": [
    "agent = IngestionAgent(\n",
    "    rss_feeds=rss_feeds,\n",
    "    api_endpoints=api_endpoints,\n",
    "    scraper_pages=scraper_pages\n",
    ")\n",
    "\n",
    "articles = agent.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88daed3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>published_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitcoin rebounds 12% from last week’s $80K low...</td>\n",
       "      <td>Bitcoin rebounded nearly 12% from its $80K low...</td>\n",
       "      <td>https://economictimes.indiatimes.com/markets/c...</td>\n",
       "      <td>RSS</td>\n",
       "      <td>2025-11-29 08:42:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F&amp;amp;O Talk| Nifty hits record high, but rall...</td>\n",
       "      <td>Markets extended their winning streak to a thi...</td>\n",
       "      <td>https://economictimes.indiatimes.com/markets/s...</td>\n",
       "      <td>RSS</td>\n",
       "      <td>2025-11-29 08:17:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bitcoin’s drawdown breaks old rule as volatili...</td>\n",
       "      <td>Bitcoin's recent downturn, despite a significa...</td>\n",
       "      <td>https://economictimes.indiatimes.com/markets/c...</td>\n",
       "      <td>RSS</td>\n",
       "      <td>2025-11-29 08:02:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Consumer demand strengthens ahead of festive &amp;...</td>\n",
       "      <td>Consumer demand is strengthening ahead of the ...</td>\n",
       "      <td>https://economictimes.indiatimes.com/markets/s...</td>\n",
       "      <td>RSS</td>\n",
       "      <td>2025-11-29 07:57:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sebi imposes 7-day ban on Prabhudas Lilladher ...</td>\n",
       "      <td>Sebi has barred Prabhudas Lilladher from takin...</td>\n",
       "      <td>https://economictimes.indiatimes.com/markets/s...</td>\n",
       "      <td>RSS</td>\n",
       "      <td>2025-11-29 07:53:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Bitcoin rebounds 12% from last week’s $80K low...   \n",
       "1  F&amp;O Talk| Nifty hits record high, but rall...   \n",
       "2  Bitcoin’s drawdown breaks old rule as volatili...   \n",
       "3  Consumer demand strengthens ahead of festive &...   \n",
       "4  Sebi imposes 7-day ban on Prabhudas Lilladher ...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Bitcoin rebounded nearly 12% from its $80K low...   \n",
       "1  Markets extended their winning streak to a thi...   \n",
       "2  Bitcoin's recent downturn, despite a significa...   \n",
       "3  Consumer demand is strengthening ahead of the ...   \n",
       "4  Sebi has barred Prabhudas Lilladher from takin...   \n",
       "\n",
       "                                                link source  \\\n",
       "0  https://economictimes.indiatimes.com/markets/c...    RSS   \n",
       "1  https://economictimes.indiatimes.com/markets/s...    RSS   \n",
       "2  https://economictimes.indiatimes.com/markets/c...    RSS   \n",
       "3  https://economictimes.indiatimes.com/markets/s...    RSS   \n",
       "4  https://economictimes.indiatimes.com/markets/s...    RSS   \n",
       "\n",
       "         published_at  \n",
       "0 2025-11-29 08:42:57  \n",
       "1 2025-11-29 08:17:52  \n",
       "2 2025-11-29 08:02:24  \n",
       "3 2025-11-29 07:57:38  \n",
       "4 2025-11-29 07:53:47  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(articles)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f889d412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Bitcoin rebounds 12% from last week’s $80K low...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad8a654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "RSS    85\n",
       "API    40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['source'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "347deddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\thang\\desktop\\hackthon_project\\tradevenv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\thang\\desktop\\hackthon_project\\tradevenv\\lib\\site-packages (4.14.2)\n",
      "Collecting lxml\n",
      "  Using cached lxml-6.0.2-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\thang\\desktop\\hackthon_project\\tradevenv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thang\\desktop\\hackthon_project\\tradevenv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thang\\desktop\\hackthon_project\\tradevenv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thang\\desktop\\hackthon_project\\tradevenv\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\thang\\desktop\\hackthon_project\\tradevenv\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\thang\\desktop\\hackthon_project\\tradevenv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Using cached lxml-6.0.2-cp311-cp311-win_amd64.whl (4.0 MB)\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-6.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15d98bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(url, link_selector=\"a\", limit=20):\n",
    "    \"\"\"\n",
    "    Extract article links from the homepage.\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # Find all links\n",
    "        for tag in soup.select(link_selector):\n",
    "            href = tag.get(\"href\")\n",
    "            if not href:\n",
    "                continue\n",
    "\n",
    "            # Clean full URLs\n",
    "            if href.startswith(\"/\"):\n",
    "                href = \"https://www.reuters.com\" + href\n",
    "\n",
    "            # Only take article-like URLs\n",
    "            if \"markets\" in href or \"business\" in href or \"finance\" in href:\n",
    "                links.append(href)\n",
    "\n",
    "            if len(links) >= limit:\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Link extraction error:\", e)\n",
    "\n",
    "    return list(set(links))  # remove duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f57c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(url):\n",
    "    \"\"\"\n",
    "    Scrapes title + main content from an article page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # Try multiple title patterns\n",
    "        title = None\n",
    "        for selector in [\"h1\", \".article-title\", \".headline\", \".title\"]:\n",
    "            t = soup.select_one(selector)\n",
    "            if t:\n",
    "                title = t.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "        # Try multiple content sections\n",
    "        content = \"\"\n",
    "        for selector in [\"article\", \".article-content\", \".story-content\", \"p\"]:\n",
    "            section = soup.select(selector)\n",
    "            if section:\n",
    "                content = \" \".join([p.get_text(\" \", strip=True) for p in section])\n",
    "                break\n",
    "\n",
    "        if title and content:\n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"content\": content,\n",
    "                \"link\": url,\n",
    "                \"source\": \"SCRAPER\",\n",
    "                \"published_at\": datetime.utcnow(),\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Scrape error for {url}: {e}\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b656498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_site(homepage_url, limit=20):\n",
    "    \"\"\"\n",
    "    Extract links from homepage and scrape each article.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting links from: {homepage_url}\")\n",
    "    links = extract_links(homepage_url, limit=limit)\n",
    "    print(f\"Found {len(links)} potential article links\")\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for link in links:\n",
    "        article = scrape_article(link)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "\n",
    "    print(f\"Scraped {len(articles)} articles successfully\")\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfd618b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting links from: https://www.reuters.com/markets/\n",
      "Found 0 potential article links\n",
      "Scraped 0 articles successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper_pages = [\n",
    "    \"https://www.reuters.com/markets/\",\n",
    "]\n",
    "\n",
    "scraped_articles = []\n",
    "for url in scraper_pages:\n",
    "    scraped_articles.extend(scrape_news_site(url, limit=25))\n",
    "\n",
    "len(scraped_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef485102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting links from: https://www.reuters.com/markets/\n",
      "Found 0 potential article links\n",
      "Scraped 0 articles successfully\n",
      "Extracting links from: https://www.cnbc.com/markets/\n",
      "Found 14 potential article links\n",
      "Scraped 0 articles successfully\n",
      "Extracting links from: https://www.business-standard.com/markets\n",
      "Found 0 potential article links\n",
      "Scraped 0 articles successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = [\n",
    "    \"https://www.reuters.com/markets/\",\n",
    "    \"https://www.cnbc.com/markets/\",\n",
    "    \"https://www.business-standard.com/markets\",\n",
    "]\n",
    "\n",
    "all_scraped = []\n",
    "for p in pages:\n",
    "    all_scraped.extend(scrape_news_site(p, limit=20))\n",
    "\n",
    "len(all_scraped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38ee0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aab54e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(url, base=None):\n",
    "    try:\n",
    "        resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        links = []\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "\n",
    "            # Ignore useless links\n",
    "            if not any(keyword in href for keyword in [\n",
    "                \"article\", \"story\", \"/markets/\", \"/business/\",\n",
    "                \"/economy/\", \"/finance/\", \"/world/\"\n",
    "            ]):\n",
    "                continue\n",
    "\n",
    "            # Make absolute URL\n",
    "            full_url = urljoin(base or url, href)\n",
    "            links.append(full_url)\n",
    "\n",
    "        return list(set(links))  # remove duplicates\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error while extracting links:\", e)\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb3cd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # Title\n",
    "        title = soup.find(\"h1\")\n",
    "        if not title:\n",
    "            return None\n",
    "\n",
    "        title = title.get_text(strip=True)\n",
    "\n",
    "        # Article body\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        content = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "\n",
    "        # Minimum length filter\n",
    "        if len(content) < 200:\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"link\": url,\n",
    "            \"source\": \"SCRAPER\",\n",
    "            \"published_at\": datetime.utcnow()\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Scraping Error:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "510c6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_from_homepage(homepage_url):\n",
    "    print(\"Extracting links from:\", homepage_url)\n",
    "    links = extract_links(homepage_url)\n",
    "\n",
    "    print(f\"Found {len(links)} potential article links\")\n",
    "\n",
    "    articles = []\n",
    "    for link in links:\n",
    "        article = scrape_article(link)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "\n",
    "    print(f\"Scraped {len(articles)} full articles\")\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3a72595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting links from: https://www.reuters.com/markets/\n",
      "Found 0 potential article links\n",
      "Scraped 0 full articles\n",
      "Extracting links from: https://www.reuters.com/business/\n",
      "Found 0 potential article links\n",
      "Scraped 0 full articles\n",
      "Extracting links from: https://www.reuters.com/world/\n",
      "Found 0 potential article links\n",
      "Scraped 0 full articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper_pages = [\n",
    "    \"https://www.reuters.com/markets/\",\n",
    "    \"https://www.reuters.com/business/\",\n",
    "    \"https://www.reuters.com/world/\"\n",
    "]\n",
    "\n",
    "scraped_articles = []\n",
    "\n",
    "for page in scraper_pages:\n",
    "    scraped_articles.extend(scrape_from_homepage(page))\n",
    "\n",
    "len(scraped_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc051a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from datetime import datetime\n",
    "\n",
    "def reuters_rss():\n",
    "    url = \"https://feeds.reuters.com/reuters/businessNews\"\n",
    "    feed = feedparser.parse(url)\n",
    "\n",
    "    articles = []\n",
    "    for entry in feed.entries:\n",
    "        articles.append({\n",
    "            \"title\": entry.title,\n",
    "            \"content\": entry.summary,\n",
    "            \"link\": entry.link,\n",
    "            \"source\": \"ReutersRSS\",\n",
    "            \"published_at\": datetime.utcnow()\n",
    "        })\n",
    "    \n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a4f9e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_articles = reuters_rss()\n",
    "len(reuters_articles), reuters_articles[:2]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tradevenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
